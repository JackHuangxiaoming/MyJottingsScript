#Python网络爬虫公开课 

######基础兴趣课入门：
![](http://i.imgur.com/ikxOqiZ.png)

	

- 爬虫能做什么- Python网络爬虫基本原理- 爬虫基础-urllib库编写爬虫- 异常处理（10分钟）- 两个爬虫的小案例


简单来说，网络爬虫就是一种能够从互联网中获取我们关注的数据的一种程序。
网络爬虫主要能做以下事情：
1、运用在搜索引擎中
2、爬取金融信息进行投资分析
3、爬取新闻数据
4、爬取图片
5、自动去网页广告
6、爬取用户联系方式，进行营销
7、爬取网站信息，进行大数据分析

![](http://i.imgur.com/kE2x78D.png)

我们可以使用urllib模块进行url的分析以及实现网络爬虫的功Python3与Python2该模块变动较大，在此我们以Python3为例讲解。
	
		import  urllib.request
		file=urllib.request.urlopen("http://www.baidu.com")
		data=file.read()
		length=len(data)
		print(length)
		html=open("F:/pycatch/baidu.html","wb")
		html.write(data)
		html.close()

我们通过网页进行一键使用处理和爬去，下载和分析使用。
我们要注意爬取过程中的异常操作。

反爬虫机制的处理方式和操作：有些网站是设置了爬虫反机制。
http://blog.csdn.net 403的机制。那么如何进行爬取，就伪装浏览器的方式进行处理。
	
		try:
		    csdnblog=urllib.request.urlopen("http://blog.csdn.net")
		except urllib.error.URLError as e:
		    #没有状态码怎么办？
		    if hasattr(e,"code"):
		        print(e.code)
		    else:
		         print("小样不嘚瑟")
		    #继续可以执行我们的代码


![](http://i.imgur.com/pioVJ8p.jpg)
实现1个爬虫的段子：如何爬取这个网站的所有段子。
代码如下：

		import urllib.request
		import re
		def getContent(url,page):
		    #模拟浏览器的模式 如何解除反爬虫机制的使用 伪装浏览器的使用 header的识别的使用
		    #useragent
		    headers=("User-Agent","Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.82 Safari/537.36")
		    opener=urllib.request.build_opener()#设置头信息
		    opener.addheaders=[headers]#添加头信息的方式进行处理
		    #将opener安装为全局的使用
		    urllib.request.install_opener(opener)
		    data=urllib.request.urlopen(url).read().decode("utf-8")
			#伪装为全局变量 啥时候去爬都可以
		#使用函数方式处理以上的操作和调用函数的方法
		#构造段子的内容的正在表达式
		#寻源码的方式进行浏览器的方法进爬取
		    content='<div class="content">(.*?)</div>'
		    #寻找所要内容 re.S多行匹配
		    contentlist=re.compile(content,re.S).findall(data)
		    for   content in contentlist:
		        content=content.replace("\n","")
		        print(content)
		        print("**********************************")
		for i in range(1,30):
		    url="http://www.qiushibaike.com/8hr/page/"+str(i)
		    getContent(url,i)

服务器的信息：

		Google Chrome	52.0.2743.82 (正式版本) （64 位）
		修订版本	49721773c8dd62828e471ca69e2c89767f98c690-refs/branch-heads/2743@{#670}
		操作系统	Windows
		Blink	537.36 (@49721773c8dd62828e471ca69e2c89767f98c690)
		JavaScript	V8 5.2.361.43
		Flash	22.0.0.209
		用户代理	Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.82 Safari/537.36
		命令行	"C:\Users\sun\AppData\Local\Google\Chrome\Application\chrome.exe" --flag-switches-begin --flag-switches-end
		可执行文件路径	C:\Users\sun\AppData\Local\Google\Chrome\Application\chrome.exe
		个人资料路径	C:\Users\sun\AppData\Local\Google\Chrome\User Data\Default
		其他变体	9f0416c1-19dd9850
		Compiler	MSVC 2015

利用浏览器的userAgent的header的信息使用网络爬虫。


![](http://i.imgur.com/OcXH6kP.jpg)

京东的产品页的爬取：

		import re
		import urllib.request
		def getPic(url,page):
		    html1=urllib.request.urlopen(url).read()
		    html1=str(html1)
		    pat1='<div id="plist".+? <div class="page clearfix">'
		    result1=re.compile(pat1).findall(html1)
		    result1=result1[0]
		    pat2='<img width="220" height="220" data-img="1" data-lazy-img="//(.+?\.jpg)">'
		    imagelist=re.compile(pat2).findall(result1)
		    x=1
		    for imageurl in imagelist:
		        imagename="F:/pycatch/propic/"+str(page)+str(x)+".jpg"
		        imageurl="http://"+imageurl
		        try:
		            urllib.request.urlretrieve(imageurl,filename=imagename)
		        except urllib.error.URLError as e:
		            if hasattr(e,"code"):
		                x+=1
		            if hasattr(e,"reason"):
		                x+=1
		        x+=1
		for i in range(1,10):
		    url="http://list.jd.com/list.html?cat=9987,653,655&page="+str(i)
		    getPic(url,i)
		print("catch over")
谢谢观看，下次再见。
![](http://i.imgur.com/nEBN6Th.jpg)